Analisis dan Diskusi Reinforcement Learning for Autonomous Control
* Analisis Hasil :
1. Bagaimana performa agen dalam mengontrol environment CartPole?
Jawab : 
Agen DQN yang dilatih menunjukkan peningkatan performa seiring dengan bertambahnya episode pelatihan. Pada awal pelatihan, agen memilih tindakan secara acak karena nilai epsilon yang tinggi (1.0). Seiringnya waktu, nilai epsilon menurun akibat epsilon decay, sehingga agen lebih sering memilih tindakan berdasarkan modelnya dibandingkan eksplorasi acak. Metrik yang dapat digunakan untuk menilai performa agen antara lain skor rata-rata (jumlah langkah sebelum tiang jatuh), konvergensi nilai epsilon (seberapa cepat agen berhenti mengeksplorasi secara acak), dan stabilitas hasil (seberapa sering agen mencapai skor tinggi di berbagai episode).	 
2. Bagaimana perubahan parameter (missal: gamma, epsilon, learning rate) mempengaruhi kinerja agen?
 Jawab :
Kinerja agen dalam mengontrol lingkungan sangat dipengaruhi oleh perubahan parameter, misalnya gamma yang dalam eksperimen ini ditetapkan sebesar 0.95. Nilai gamma yang terlalu rendah, seperti 0.5, membuat agen hanya fokus pada reward jangka pendek tanpa mempertimbangkan konsekuensi di masa depan. Sebaliknya, jika terlalu tinggi, seperti 0.99, agen cenderung lebih memperhatikan reward jangka panjang, tetapi dapat menyebabkan eksplorasi yang kurang optimal. Kemudian untuk epsilon, awalnya, nilai epsilon sebesar 1.0 digunakan untuk memastikan eksplorasi maksimal, lalu berkurang secara bertahap sesuai epsilon decay. Jika epsilon decay terlalu cepat, agen dapat berhenti mengeksplorasi sebelum menemukan strategi optimal. Namun, jika terlalu lambat, agen akan terlalu lama melakukan tindakan acak, sehingga menghambat proses pembelajaran. Learning Rate sebesar 0.001 juga digunakan agar model belajar dengan kecepatan yang moderat. Jika learning rate terlalu tinggi, model menjadi tidak stabil karena terlalu cepat menyesuaikan bobotnya terhadap perubahan kecil. Sebaliknya, jika terlalu rendah, pembelajaran menjadi sangat lambat, sehingga agen sulit mencapai performa optimal. 
3. Apa tantangan yang muncul selama pelatihan agen RL?
 Jawab :
Tantangan yang muncul selama pelatihan agen RL yaitu DQN bisa mengalami catastrophic forgetting, yaitu model lupa strategi lama karena tidak ada mekanisme memisahkan pengalaman lama dengan yang baru. Kemudian menyeimbangkan antara eksplorasi (mencoba tindakan baru) dan eksploitasi (menggunakan strategi yang sudah terbukti efektif) adalah tantangan utama dalam RL. Jika agen hanya dilatih pada satu environment (misalnya CartPole), kemungkinan tidak bisa beradaptasi pada environment lain seperti LunarLander atau MountainCar.

* Diskusi :
1. Apa perbedaan utama antara Reinforcement Learning dan metode supervised learning dalam sistem kendali?
Jawab : 
Perbedaan utama dari Reinforcement Learning yaitu Agen belajar dari interaksi dengan lingkungan, tidak membutuhkan label eksplisit, hanya feedback berupa reward atau penalty, serta digunakan untuk sistem kendali yang dinamis dan tidak memiliki jawaban yang tetap, seperti robotika dan kendaraan otonom. Sedangkan metode supervised learning yaitu membutuhkan data berlabel (input-output), model belajar dari contoh yang sudah diberikan manusia, serta cocok untuk tugas seperti klasifikasi gambar atau prediksi harga saham.
2. Bagaimana strategi eksplorasi (exploration) dan eksploitasi (exploitation) dapat dioptimalkan pada agen RL?
Jawab : 
Strategi eksplorasi dan eksploitasi pada agen Reinforcement Learning (RL) dapat dioptimalkan dengan pendekatan yang seimbang agar agen dapat menemukan strategi terbaik tanpa terlalu lama melakukan eksplorasi acak. Epsilon-Greedy Strategy adalah metode umum di mana agen memilih tindakan secara acak dengan probabilitas ?? dan memilih tindakan terbaik dengan probabilitas 1-??. Untuk meningkatkan eksploitasi seiring waktu, digunakan Decay Epsilon, yang secara bertahap mengurangi nilai epsilon agar agen lebih sering memilih tindakan berdasarkan pengalaman sebelumnya. Selain itu, metode Upper Confidence Bound (UCB) dapat digunakan untuk mengatasi eksplorasi yang tidak efisien dengan mempertimbangkan ketidakpastian nilai tindakan, sehingga agen lebih cenderung mencoba tindakan yang belum cukup dievaluasi. 
3. Potensi aplikasi lain dari RL dalam sistem kendali nyata apa saja yang dapat diimplementasikan?
Jawab : 
RL memuiliki potensi besar dalam berbagai bidang, dalam kendaraan otonom RL dapat digunakan untuk navigasi dan pengambilan keputusan pada mobil self-driving. Pada Robotika Industri mengoptimalkan pergerakan robot di pabrik untuk meningkatkan efisiensi produksi. Pada Manajemen Energi, RL bisa digunakan untuk mengoptimalkan distribusi daya listrik agar lebih hemat energi. Penggunaan RL juga dapat digunakan untuk strategi trading otomatis di pasar saham. Serta dapat mengoptimalkan distribusi lalu lintas dengan sistem AI berbasis RL.
	
